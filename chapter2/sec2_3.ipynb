{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Operations\n",
    "=======\n",
    "\n",
    "Naive vs Vectorised\n",
    "------\n",
    "\n",
    "Most deep learning algorithms can be reduced to a set of tensor operations.\n",
    "\n",
    "The FC layer we used earlier in the course:\n",
    "\n",
    "`keras.layers.Dense(512, activation='relu')`\n",
    "\n",
    "Can be viewed as a function, the output of which is:\n",
    "\n",
    "`output = relu(dot(W, input) + b)`\n",
    "\n",
    "Where W and b are our learned values.\n",
    "\n",
    "Both relu and addition are element wise operations.\n",
    "We can implement these operations naively with for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.23 ms ± 25.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "3.37 ms ± 26.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2\n",
    "    \n",
    "    x = x.copy() #avoids overwriting the original tensor\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x\n",
    "\n",
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = x[i, j] + y[i, j]\n",
    "    return x\n",
    "\n",
    "\n",
    "x = np.ones((1000,10))\n",
    "y = np.ones((1000,10))\n",
    "\n",
    "%timeit naive_relu(x)\n",
    "%timeit naive_add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this tends to be slow. Luckily these elementwise operations are amenable to vectorized implementations. Under the hood numpy uses library such as BLAS to perform these operations at super quick speeds. Rather than writing the for loops explicitly we can instead write them like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.34 µs ± 48.6 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "3.65 µs ± 16.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def vec_relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def vec_add(x, y):\n",
    "    return x + y\n",
    "\n",
    "%timeit vec_relu(x)\n",
    "%timeit vec_add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting\n",
    "------\n",
    "\n",
    "Our naive implementation supports the addition of 2 tensors of the same shape. But what happens in the vectorised example if the shapes of the tensors differ?\n",
    "\n",
    "When possible, if no ambiguity exists the smaller tensor will be *broadcasted* into the larger tensor. First axes are added to the smaller tensor so it matches the size of the bigger. Then the smaller tensor is repeated along these new axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.27555479 1.3224021  1.27638818 1.43912916 1.25725977 1.69828085\n",
      "  1.73929516 1.10518811 1.04255149 1.69539249]\n",
      " [1.27555479 1.3224021  1.27638818 1.43912916 1.25725977 1.69828085\n",
      "  1.73929516 1.10518811 1.04255149 1.69539249]\n",
      " [1.27555479 1.3224021  1.27638818 1.43912916 1.25725977 1.69828085\n",
      "  1.73929516 1.10518811 1.04255149 1.69539249]\n",
      " [1.27555479 1.3224021  1.27638818 1.43912916 1.25725977 1.69828085\n",
      "  1.73929516 1.10518811 1.04255149 1.69539249]\n",
      " [1.27555479 1.3224021  1.27638818 1.43912916 1.25725977 1.69828085\n",
      "  1.73929516 1.10518811 1.04255149 1.69539249]\n",
      " [1.27555479 1.3224021  1.27638818 1.43912916 1.25725977 1.69828085\n",
      "  1.73929516 1.10518811 1.04255149 1.69539249]\n",
      " [1.27555479 1.3224021  1.27638818 1.43912916 1.25725977 1.69828085\n",
      "  1.73929516 1.10518811 1.04255149 1.69539249]\n",
      " [1.27555479 1.3224021  1.27638818 1.43912916 1.25725977 1.69828085\n",
      "  1.73929516 1.10518811 1.04255149 1.69539249]\n",
      " [1.27555479 1.3224021  1.27638818 1.43912916 1.25725977 1.69828085\n",
      "  1.73929516 1.10518811 1.04255149 1.69539249]\n",
      " [1.27555479 1.3224021  1.27638818 1.43912916 1.25725977 1.69828085\n",
      "  1.73929516 1.10518811 1.04255149 1.69539249]\n",
      " [1.27555479 1.3224021  1.27638818 1.43912916 1.25725977 1.69828085\n",
      "  1.73929516 1.10518811 1.04255149 1.69539249]\n",
      " [1.27555479 1.3224021  1.27638818 1.43912916 1.25725977 1.69828085\n",
      "  1.73929516 1.10518811 1.04255149 1.69539249]]\n"
     ]
    }
   ],
   "source": [
    "x = np.ones((12, 10))\n",
    "y = np.random.rand(10)\n",
    "\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can tell the smaller tensor of shape (10) is first increased to shape (1, 10). It is then repeated 12 times along this new axis so it is of the shape (12, 10) before the addition occurs.\n",
    "\n",
    "In terms of efficiency this new tensor is never explicitly created as that would be inneficient. It occurs only at the algorithmic level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Dot\n",
    "-----\n",
    "\n",
    "The dot product or tensor product is another common operation `x.y`. DO NOT CONFUSE IT WITH THE ELEMENT WISE PRODUCT `x*y`!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.84750744  8.38681162  8.85569691  7.64911099  5.50945767  6.94383679\n",
      "   6.44136261  6.26346693  6.16513987  7.44493172]\n",
      " [ 7.11336813  6.76593849  7.85222978  6.97592458  5.27223657  6.77781766\n",
      "   5.33321827  6.53015894  4.95191001  6.12925617]\n",
      " [10.34004842  9.09215371  9.18170543  8.70208187  7.10557054  8.54436447\n",
      "   8.47307778  8.30695393  6.79804201  9.74560873]\n",
      " [ 9.6571777   9.41904653 10.4058656  10.09287886  7.0616049   8.64824469\n",
      "   8.35083146  9.28013298  6.15444776 10.25322142]\n",
      " [ 6.82626766  7.01434511  8.08307291  6.88862745  5.28847955  6.09679284\n",
      "   6.59041278  6.12721618  5.15854444  6.9232241 ]\n",
      " [ 7.23440819  7.48213174  9.01365618  7.84881519  6.34782022  7.85440641\n",
      "   6.43102044  6.4522272   5.58419033  7.93090153]\n",
      " [ 8.62089136  8.45463802  9.33274931  8.02400947  6.27913525  7.49713346\n",
      "   6.29127201  7.85910015  5.49322854  7.86559291]\n",
      " [ 8.9427716   8.08770469  8.30988043  8.64238506  5.98214055  7.76750047\n",
      "   8.07846186  7.50257905  6.69012934  8.37185537]\n",
      " [ 6.75770744  7.81719568  8.36435059  7.70334558  5.70010255  7.04041613\n",
      "   6.7005413   6.70343329  5.07544871  7.89518422]\n",
      " [ 8.13551345  7.86096306  8.07696571  8.19887878  6.20145371  7.94046339\n",
      "   7.47931135  7.51501229  6.20867521  8.81253558]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(10, 32)\n",
    "y = np.random.rand(32, 10)\n",
    "\n",
    "print(np.dot(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is this doing exactly? Lets start with a naive vector implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    \n",
    "    z = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here for a vector the dot product is the sum of the element wise product which is a scalar.\n",
    "Between a matrix x and a vector y it's more complex. The dot product of `x.y` becomes the dot product between y and the rows of x. This can be implemented naively as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[0]):\n",
    "            z[i] += x[i, j] + y[j]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Or by re-using our old code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i], y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note* now that ndim>1 this is no longer symmetric. `x.y` is not the same as `y.x`.\n",
    "\n",
    "And finally a matrix dot product between x and y produces a matrix which is the dot product of all rows of x with all columns of y. Therefore the numbers of columns in x must equal the rows in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            z[i, j] = naive_vector_dot(x[i], y[:, j])\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or again by re-using our old code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_matrix_vector_dot(y.T, x[i])\n",
    "    return z\n",
    "\n",
    "print(np.isclose(naive_matrix_dot(x, y), np.dot(x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference the book for a nice diagram of this interaction. if x is of shape (a, b) and y of (b, c) then the result will be (a, c).\n",
    "\n",
    "This approach genralises to higher dimensions, for example.\n",
    "(a, b, c, d) . (d,) -> (a, b, c)\n",
    "(a, b, c, d) . (d, e) -> (a, b, c, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Reshaping\n",
    "-------\n",
    "\n",
    "Another essential operation used throughout deep learning is tensor reshaping. This was used as part of our pre-processing of the MNIST dataset.\n",
    "`train_images = train_images.reshape((train_images.shape[0], train_images.shape[1]*train_images.shape[2]))`\n",
    "This operation is easiest to explain with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n",
      "(6,)\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n",
      "(6, 1)\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "(3, 2)\n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0, 1, 2, 3, 4, 5])\n",
    "print(x)\n",
    "print(x.shape)\n",
    "x = x.reshape((6, 1))\n",
    "print(x)\n",
    "print(x.shape)\n",
    "x = x.reshape((3, 2))\n",
    "print(x)\n",
    "print(x.shape)\n",
    "x = x.reshape((2, 3))\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a special case of reshape known as the *transpose*. This operations swaps the rows and columns of a matrix so that x["
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
